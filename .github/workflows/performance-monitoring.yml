name: Performance Monitoring

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      baseline_comparison:
        description: 'Compare against baseline'
        required: false
        default: true
        type: boolean

env:
  PYTHON_VERSION: '3.10'

jobs:
  performance-regression-test:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      issues: write
      pull-requests: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest-benchmark

    - name: Download baseline results
      uses: actions/download-artifact@v4
      if: github.event.inputs.baseline_comparison == 'true' || github.event_name == 'schedule'
      with:
        name: benchmark-baseline
        path: baseline/

    - name: Run performance benchmarks
      run: |
        python benchmark_performance.py \
          --comprehensive \
          --json-output current-results.json \
          --baseline baseline/benchmark-results.json \
          --regression-threshold 0.05

    - name: Generate performance report
      run: |
        python scripts/generate_performance_report.py \
          --current current-results.json \
          --baseline baseline/benchmark-results.json \
          --output performance-report.md

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.run_id }}
        path: |
          current-results.json
          performance-report.md

    - name: Update baseline (main branch only)
      if: github.ref == 'refs/heads/main' && github.event_name == 'schedule'
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-baseline
        path: current-results.json

    - name: Create performance issue on regression
      if: failure() && github.event_name == 'schedule'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('performance-report.md', 'utf8');

          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `ðŸš¨ Performance Regression Detected - ${new Date().toISOString().split('T')[0]}`,
            body: `## Performance Regression Alert

          A performance regression has been detected in the latest benchmark run.

          ### Details
          ${report}

          ### Actions Required
          - Review recent changes that may have caused the regression
          - Optimize affected components
          - Update performance baselines if changes are expected

          ### Benchmark Run
          - Commit: ${context.sha}
          - Run ID: ${context.runId}
          - Timestamp: ${new Date().toISOString()}

          @github/team-maintainers`,
            labels: ['performance', 'regression', 'urgent']
          });

  memory-leak-detection:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install memory-profiler

    - name: Run memory leak tests
      run: |
        python -m pytest tests/test_memory_leaks.py -v --tb=short --durations=10

    - name: Upload memory profiles
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: memory-profiles-${{ github.run_id }}
        path: memory-profiles/

  api-performance-monitoring:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install locust

    - name: Run API load testing
      run: |
        # Start API server
        python -m uvicorn code_explainer.api.server:app --host 0.0.0.0 --port 8000 &
        API_PID=$!
        sleep 15

        # Run load tests
        locust --headless --users 100 --spawn-rate 10 --run-time 2m --host http://localhost:8000 --csv results

        # Stop API server
        kill $API_PID || true

    - name: Upload load test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: load-test-results-${{ github.run_id }}
        path: results_*.csv