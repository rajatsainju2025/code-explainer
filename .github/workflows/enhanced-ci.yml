name: Enhanced CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - 'LICENSE'
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC

env:
  PYTHON_VERSION: "3.11"
  CACHE_VERSION: v1

jobs:
  # Pre-commit checks
  pre-commit:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    - uses: pre-commit/action@v3.0.0

  # Quality checks with matrix testing
  quality-checks:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.8", "3.9", "3.10", "3.11"]
        check-type: [lint, type, security]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/pip
          ~/.cache/pre-commit
        key: ${{ env.CACHE_VERSION }}-${{ runner.os }}-py${{ matrix.python-version }}-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
        restore-keys: |
          ${{ env.CACHE_VERSION }}-${{ runner.os }}-py${{ matrix.python-version }}-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    
    - name: Run linting
      if: matrix.check-type == 'lint'
      run: |
        ruff check src/ tests/ evals/ --output-format=github
        black --check src/ tests/ evals/
        isort --check-only src/ tests/ evals/
    
    - name: Run type checking
      if: matrix.check-type == 'type'
      run: |
        mypy src/ evals/ --strict --ignore-missing-imports
    
    - name: Run security checks
      if: matrix.check-type == 'security'
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        safety check --json --output safety-report.json || true
        semgrep --config=auto src/ --json --output=semgrep-report.json || true
    
    - name: Upload security artifacts
      if: matrix.check-type == 'security'
      uses: actions/upload-artifact@v3
      with:
        name: security-reports-py${{ matrix.python-version }}
        path: "*-report.json"

  # Comprehensive testing
  test-suite:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.8", "3.9", "3.10", "3.11"]
        test-type: [unit, integration, performance]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ env.CACHE_VERSION }}-${{ runner.os }}-py${{ matrix.python-version }}-${{ hashFiles('**/requirements.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    
    - name: Run unit tests
      if: matrix.test-type == 'unit'
      run: |
        pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html
    
    - name: Run integration tests
      if: matrix.test-type == 'integration'
      run: |
        pytest tests/integration/ -v --durations=10
    
    - name: Run performance tests
      if: matrix.test-type == 'performance'
      run: |
        pytest tests/performance/ -v --benchmark-only --benchmark-json=benchmark.json
    
    - name: Upload coverage to Codecov
      if: matrix.test-type == 'unit' && matrix.python-version == env.PYTHON_VERSION
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
    
    - name: Upload test artifacts
      uses: actions/upload-artifact@v3
      with:
        name: test-results-${{ matrix.test-type }}-py${{ matrix.python-version }}
        path: |
          htmlcov/
          benchmark.json
          pytest-report.xml

  # Evaluation smoke tests
  eval-smoke-tests:
    runs-on: ubuntu-latest
    needs: [quality-checks]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    
    - name: Run minimal evaluation
      run: |
        make eval-minimal
        
    - name: Validate evaluation outputs
      run: |
        python -c "
        import json
        from pathlib import Path
        
        # Check that outputs exist
        results_dir = Path('results/minimal_eval')
        assert results_dir.exists(), 'Results directory not found'
        assert (results_dir / 'metrics.json').exists(), 'Metrics file not found'
        assert (results_dir / 'run_manifest.json').exists(), 'Manifest not found'
        
        # Validate metrics format
        with open(results_dir / 'metrics.json') as f:
            metrics = json.load(f)
        assert 'bleu_score' in metrics, 'BLEU score missing'
        assert 'avg_latency' in metrics, 'Latency missing'
        assert metrics['num_samples'] > 0, 'No samples processed'
        
        print('âœ… Evaluation smoke test passed')
        "
    
    - name: Upload evaluation artifacts
      uses: actions/upload-artifact@v3
      with:
        name: eval-smoke-test-results
        path: results/minimal_eval/

  # Build and publish
  build-and-publish:
    runs-on: ubuntu-latest
    needs: [test-suite, eval-smoke-tests]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for version tags
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine
    
    - name: Build package
      run: |
        python -m build
    
    - name: Validate package
      run: |
        twine check dist/*
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: dist-packages
        path: dist/

  # Docker build and security scan
  docker-security:
    runs-on: ubuntu-latest
    needs: [quality-checks]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: false
        tags: code-explainer:test
        cache-from: type=gha
        cache-to: type=gha,mode=max
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'code-explainer:test'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  # Performance regression detection
  performance-regression:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    
    - name: Run current branch benchmark
      run: |
        make eval-minimal
        cp results/minimal_eval/metrics.json current-metrics.json
    
    - name: Checkout main branch
      run: |
        git checkout main
        pip install -e .[dev]
    
    - name: Run main branch benchmark
      run: |
        make eval-minimal
        cp results/minimal_eval/metrics.json main-metrics.json
    
    - name: Compare performance
      run: |
        python -c "
        import json
        
        with open('main-metrics.json') as f:
            main_metrics = json.load(f)
        with open('current-metrics.json') as f:
            current_metrics = json.load(f)
        
        # Check for significant regressions
        latency_change = (current_metrics['avg_latency'] - main_metrics['avg_latency']) / main_metrics['avg_latency'] * 100
        
        if latency_change > 20:  # 20% regression threshold
            print(f'âŒ Performance regression detected: {latency_change:.1f}% latency increase')
            exit(1)
        else:
            print(f'âœ… Performance check passed: {latency_change:.1f}% latency change')
        "

  # Notification and reporting
  notify-results:
    runs-on: ubuntu-latest
    needs: [test-suite, eval-smoke-tests, build-and-publish, docker-security]
    if: always()
    
    steps:
    - name: Report status
      run: |
        echo "## ðŸš€ CI/CD Pipeline Results" >> $GITHUB_STEP_SUMMARY
        echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Quality Checks | ${{ needs.quality-checks.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Test Suite | ${{ needs.test-suite.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Eval Smoke Tests | ${{ needs.eval-smoke-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Build & Publish | ${{ needs.build-and-publish.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Docker Security | ${{ needs.docker-security.result }} |" >> $GITHUB_STEP_SUMMARY
