# ICML Experiment Configuration
# This YAML file contains all experimental settings for reproducing ICML paper results

# Experiment metadata
experiment_name: "codeexplaingpt_icml_2025"
experiment_version: "1.0.0"
description: "Multi-Agent Retrieval-Augmented Code Explanation with Language-Adaptive Processing"
authors: ["Research Team"]
institution: "Research Institution"
contact: "research@institution.edu"

# Reproducibility settings
seed: 42
deterministic: true
num_runs: 5  # For statistical significance
cross_validation_folds: 5

# Hardware requirements
hardware:
  min_gpu_memory: "16GB"
  recommended_gpu: "A100"
  min_cpu_cores: 8
  min_ram: "32GB"

# Dataset configuration
datasets:
  primary:
    - name: "concode"
      path: "data/concode"
      languages: ["java"]
      split_ratio: [0.8, 0.1, 0.1]  # train/val/test
      max_samples: 10000
    - name: "codexglue"
      path: "data/codexglue"
      languages: ["python", "java", "javascript", "c++", "go", "ruby"]
      split_ratio: [0.8, 0.1, 0.1]
      max_samples: 50000
    - name: "codesearchnet"
      path: "data/codesearchnet"
      languages: ["python", "java", "javascript", "php", "go", "ruby"]
      split_ratio: [0.8, 0.1, 0.1]
      max_samples: 100000

  validation:
    - name: "code_docstring_corpus"
      path: "data/code_docstring_corpus"
      languages: ["python"]
      use_for: ["validation"]
    - name: "stackoverflow_qa"
      path: "data/stackoverflow_qa"
      languages: ["multiple"]
      use_for: ["human_eval"]

# Model configurations
models:
  primary:
    name: "codeexplaingpt"
    architecture: "multi_agent_rag"
    pretrained_base: "microsoft/codebert-base"

  baselines:
    - name: "codebert"
      architecture: "encoder_decoder"
      pretrained_base: "microsoft/codebert-base"
      config:
        max_length: 512
        num_beams: 4

    - name: "codet5"
      architecture: "encoder_decoder"
      pretrained_base: "Salesforce/codet5-base"
      config:
        max_length: 512
        num_beams: 4

    - name: "graphcodebert"
      architecture: "graph_encoder_decoder"
      pretrained_base: "microsoft/graphcodebert-base"
      config:
        max_length: 512
        num_beams: 4

    - name: "gpt35_turbo"
      architecture: "api"
      provider: "openai"
      model_id: "gpt-3.5-turbo"
      config:
        temperature: 0.1
        max_tokens: 512

    - name: "gpt4"
      architecture: "api"
      provider: "openai"
      model_id: "gpt-4"
      config:
        temperature: 0.1
        max_tokens: 512

# Training configuration
training:
  optimizer: "adamw"
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 1000
  max_epochs: 10
  early_stopping_patience: 3
  gradient_clipping: 1.0

  batch_size:
    train: 16
    eval: 32

  mixed_precision: true
  gradient_checkpointing: true

  # Multi-agent specific
  agent_coordination:
    method: "hierarchical"
    communication_rounds: 3
    consensus_threshold: 0.8

# Retrieval configuration
retrieval:
  index_type: "faiss"
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  similarity_metric: "cosine"
  top_k: 5

  knowledge_base:
    code_snippets: "data/knowledge_base/code_snippets.jsonl"
    documentation: "data/knowledge_base/documentation.jsonl"
    examples: "data/knowledge_base/examples.jsonl"

# Evaluation configuration
evaluation:
  metrics:
    automatic:
      - "bleu"
      - "rouge_l"
      - "bert_score"
      - "code_bleu"
      - "semantic_similarity"
      - "code_similarity"
      - "explanation_coherence"
      - "technical_accuracy"

    human_eval:
      - "clarity"
      - "accuracy"
      - "completeness"
      - "helpfulness"
      - "readability"

  human_evaluation:
    num_evaluators: 3
    num_samples: 1000
    inter_annotator_agreement_threshold: 0.7
    evaluation_platform: "amazon_mturk"

  error_analysis:
    categories:
      - "syntax_errors"
      - "semantic_errors"
      - "incomplete_explanations"
      - "hallucinations"
      - "language_confusion"

  ablation_studies:
    - name: "no_retrieval"
      description: "Remove retrieval component"
      changes: {"retrieval_enabled": false}

    - name: "single_agent"
      description: "Use single agent instead of multi-agent"
      changes: {"num_agents": 1}

    - name: "no_language_adaptation"
      description: "Remove language-specific processing"
      changes: {"language_adaptive": false}

    - name: "no_symbolic_analysis"
      description: "Remove symbolic code analysis"
      changes: {"symbolic_analysis": false}

# Statistical analysis
statistics:
  significance_test: "wilcoxon"
  confidence_level: 0.95
  effect_size_measure: "cohen_d"
  multiple_comparison_correction: "bonferroni"
  bootstrap_iterations: 10000

# Output configuration
output:
  results_dir: "results/icml_experiment"
  save_predictions: true
  save_model_checkpoints: true
  save_intermediate_results: true

  plots:
    - "learning_curves"
    - "error_distribution"
    - "language_performance"
    - "ablation_results"
    - "human_eval_correlation"

  tables:
    - "main_results"
    - "ablation_results"
    - "human_evaluation"
    - "error_analysis"
    - "statistical_significance"

  latex_output: true

# Hyperparameter search (optional)
hyperparameter_search:
  enabled: false
  method: "optuna"
  n_trials: 100
  parameters:
    learning_rate: [1e-6, 1e-4]
    batch_size: [8, 16, 32]
    num_agents: [2, 3, 4, 5]
    retrieval_top_k: [3, 5, 10]

# Compute resources
compute:
  distributed_training: false
  num_gpus: 1
  mixed_precision: true
  compile_model: true

# Logging and monitoring
logging:
  level: "INFO"
  wandb:
    enabled: true
    project: "codeexplaingpt_icml"
    tags: ["icml2025", "code_explanation", "multi_agent"]

  tensorboard:
    enabled: true
    log_dir: "logs/tensorboard"

  file_logging:
    enabled: true
    log_file: "logs/experiment.log"

# Quality assurance
quality_checks:
  code_quality:
    - "black"
    - "isort"
    - "flake8"
    - "mypy"

  data_quality:
    - "duplicate_detection"
    - "format_validation"
    - "language_detection"
    - "quality_filtering"

  model_quality:
    - "gradient_checking"
    - "loss_validation"
    - "output_validation"
    - "memory_profiling"

# Publication settings
publication:
  paper_template: "icml2025"
  supplementary_material: true
  code_release: true
  data_release: true  # Subject to licensing

  reproducibility_checklist:
    - "algorithm_description"
    - "hyperparameter_specification"
    - "dataset_description"
    - "evaluation_protocol"
    - "statistical_analysis"
    - "computational_requirements"
    - "code_availability"
